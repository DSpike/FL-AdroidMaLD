{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Android Malware Detection using Federated Learning\n",
      "-------------------------------------------------\n",
      "\n",
      "Loading Android malware datasets...\n",
      "Found 12 files matching the pattern\n",
      "Maximum feature dimension across all files: 121\n",
      "Padded features in C:\\Users\\USER\\Documents\\NTUST\\Conference_Workshop_Seminar\\Android\\Dataset\\AndMal2020-dynamic-BeforeAndAfterReboot\\Cleaned_Files\\normalized_dataset\\cleaned_Backdoor_after_reboot_Cat.csv from 110 to 121\n",
      "Padded features in C:\\Users\\USER\\Documents\\NTUST\\Conference_Workshop_Seminar\\Android\\Dataset\\AndMal2020-dynamic-BeforeAndAfterReboot\\Cleaned_Files\\normalized_dataset\\cleaned_FileInfector_after_reboot_Cat.csv from 97 to 121\n",
      "Padded features in C:\\Users\\USER\\Documents\\NTUST\\Conference_Workshop_Seminar\\Android\\Dataset\\AndMal2020-dynamic-BeforeAndAfterReboot\\Cleaned_Files\\normalized_dataset\\cleaned_PUA_after_reboot_Cat.csv from 104 to 121\n",
      "Padded features in C:\\Users\\USER\\Documents\\NTUST\\Conference_Workshop_Seminar\\Android\\Dataset\\AndMal2020-dynamic-BeforeAndAfterReboot\\Cleaned_Files\\normalized_dataset\\cleaned_Ransomware_after_reboot_Cat.csv from 101 to 121\n",
      "Padded features in C:\\Users\\USER\\Documents\\NTUST\\Conference_Workshop_Seminar\\Android\\Dataset\\AndMal2020-dynamic-BeforeAndAfterReboot\\Cleaned_Files\\normalized_dataset\\cleaned_Riskware_after_reboot_Cat.csv from 113 to 121\n",
      "Padded features in C:\\Users\\USER\\Documents\\NTUST\\Conference_Workshop_Seminar\\Android\\Dataset\\AndMal2020-dynamic-BeforeAndAfterReboot\\Cleaned_Files\\normalized_dataset\\cleaned_Scareware_after_reboot_Cat.csv from 101 to 121\n",
      "Padded features in C:\\Users\\USER\\Documents\\NTUST\\Conference_Workshop_Seminar\\Android\\Dataset\\AndMal2020-dynamic-BeforeAndAfterReboot\\Cleaned_Files\\normalized_dataset\\cleaned_Trojan_after_reboot_Cat.csv from 117 to 121\n",
      "Padded features in C:\\Users\\USER\\Documents\\NTUST\\Conference_Workshop_Seminar\\Android\\Dataset\\AndMal2020-dynamic-BeforeAndAfterReboot\\Cleaned_Files\\normalized_dataset\\cleaned_Trojan_Banker_after_reboot_Cat.csv from 102 to 121\n",
      "Padded features in C:\\Users\\USER\\Documents\\NTUST\\Conference_Workshop_Seminar\\Android\\Dataset\\AndMal2020-dynamic-BeforeAndAfterReboot\\Cleaned_Files\\normalized_dataset\\cleaned_Trojan_Dropper_after_reboot_Cat.csv from 108 to 121\n",
      "Padded features in C:\\Users\\USER\\Documents\\NTUST\\Conference_Workshop_Seminar\\Android\\Dataset\\AndMal2020-dynamic-BeforeAndAfterReboot\\Cleaned_Files\\normalized_dataset\\cleaned_Trojan_SMS_after_reboot_Cat.csv from 107 to 121\n",
      "Padded features in C:\\Users\\USER\\Documents\\NTUST\\Conference_Workshop_Seminar\\Android\\Dataset\\AndMal2020-dynamic-BeforeAndAfterReboot\\Cleaned_Files\\normalized_dataset\\cleaned_Trojan_Spy_after_reboot_Cat.csv from 106 to 121\n",
      "Loaded 22029 samples across 12 malware families\n",
      "Each sample has 121 features\n",
      "Malware families: ['Adware' 'Backdoor' 'FileInfector' 'PUA' 'Ransomware' 'Riskware'\n",
      " 'Scareware' 'Trojan' 'Trojan_Banker' 'Trojan_Dropper' 'Trojan_SMS'\n",
      " 'Trojan_Spy']\n",
      "\n",
      "Dataset Statistics:\n",
      "Number of samples: 22029\n",
      "Number of features: 121\n",
      "Number of malware families: 12\n",
      "\n",
      "Initializing Federated Learning Server...\n",
      "\n",
      "Starting Federated Learning Training...\n",
      "\n",
      "Client Dataset Sizes:\n",
      "Client 0: 5874 samples\n",
      "Client 1: 5874 samples\n",
      "Client 2: 5875 samples\n",
      "\n",
      "Communication Round 1\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "Client 0, Epoch 1/5, Loss: 0.1755\n",
      "Client 0, Epoch 2/5, Loss: 0.0112\n",
      "Client 0, Epoch 3/5, Loss: 0.0008\n",
      "Client 0, Epoch 4/5, Loss: 0.0002\n",
      "Client 0, Epoch 5/5, Loss: 0.0002\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "Client 1, Epoch 1/5, Loss: 0.1796\n",
      "Client 1, Epoch 2/5, Loss: 0.0066\n",
      "Client 1, Epoch 3/5, Loss: 0.0105\n",
      "Client 1, Epoch 4/5, Loss: 0.0052\n",
      "Client 1, Epoch 5/5, Loss: 0.0005\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "Client 2, Epoch 1/5, Loss: 0.1523\n",
      "Client 2, Epoch 2/5, Loss: 0.0151\n",
      "Client 2, Epoch 3/5, Loss: 0.0068\n",
      "Client 2, Epoch 4/5, Loss: 0.0009\n",
      "Client 2, Epoch 5/5, Loss: 0.0038\n",
      "Client Accuracies: ['100.00%', '100.00%', '100.00%']\n",
      "Global Validation Accuracy: 99.84%\n",
      "Global Validation Loss: 0.0053\n",
      "\n",
      "Communication Round 2\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "Client 0, Epoch 1/5, Loss: 0.0209\n",
      "Client 0, Epoch 2/5, Loss: 0.0035\n",
      "Client 0, Epoch 3/5, Loss: 0.0129\n",
      "Client 0, Epoch 4/5, Loss: 0.0122\n",
      "Client 0, Epoch 5/5, Loss: 0.0054\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "Client 1, Epoch 1/5, Loss: 0.0102\n",
      "Client 1, Epoch 2/5, Loss: 0.0168\n",
      "Client 1, Epoch 3/5, Loss: 0.0086\n",
      "Client 1, Epoch 4/5, Loss: 0.0077\n",
      "Client 1, Epoch 5/5, Loss: 0.0088\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "Client 2, Epoch 1/5, Loss: 0.0179\n",
      "Client 2, Epoch 2/5, Loss: 0.0103\n",
      "Client 2, Epoch 3/5, Loss: 0.0107\n",
      "Client 2, Epoch 4/5, Loss: 0.0038\n",
      "Client 2, Epoch 5/5, Loss: 0.0001\n",
      "Client Accuracies: ['99.97%', '100.00%', '100.00%']\n",
      "Global Validation Accuracy: 99.98%\n",
      "Global Validation Loss: 0.0007\n",
      "\n",
      "Communication Round 3\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "Client 0, Epoch 1/5, Loss: 0.0096\n",
      "Client 0, Epoch 2/5, Loss: 0.0086\n",
      "Client 0, Epoch 3/5, Loss: 0.0046\n",
      "Client 0, Epoch 4/5, Loss: 0.0126\n",
      "Client 0, Epoch 5/5, Loss: 0.0082\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "Client 1, Epoch 1/5, Loss: 0.0037\n",
      "Client 1, Epoch 2/5, Loss: 0.0151\n",
      "Client 1, Epoch 3/5, Loss: 0.0046\n",
      "Client 1, Epoch 4/5, Loss: 0.0078\n",
      "Client 1, Epoch 5/5, Loss: 0.0004\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "Client 2, Epoch 1/5, Loss: 0.0020\n",
      "Client 2, Epoch 2/5, Loss: 0.0241\n",
      "Client 2, Epoch 3/5, Loss: 0.0042\n",
      "Client 2, Epoch 4/5, Loss: 0.0053\n",
      "Client 2, Epoch 5/5, Loss: 0.0026\n",
      "Client Accuracies: ['99.97%', '100.00%', '100.00%']\n",
      "Global Validation Accuracy: 99.98%\n",
      "Global Validation Loss: 0.0011\n",
      "\n",
      "Communication Round 4\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "Client 0, Epoch 1/5, Loss: 0.0005\n",
      "Client 0, Epoch 2/5, Loss: 0.0013\n",
      "Client 0, Epoch 3/5, Loss: 0.0138\n",
      "Client 0, Epoch 4/5, Loss: 0.0097\n",
      "Client 0, Epoch 5/5, Loss: 0.0014\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "Client 1, Epoch 1/5, Loss: 0.0000\n",
      "Client 1, Epoch 2/5, Loss: 0.0112\n",
      "Client 1, Epoch 3/5, Loss: 0.0023\n",
      "Client 1, Epoch 4/5, Loss: 0.0110\n",
      "Client 1, Epoch 5/5, Loss: 0.0046\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "Client 2, Epoch 1/5, Loss: 0.0021\n",
      "Client 2, Epoch 2/5, Loss: 0.0033\n",
      "Client 2, Epoch 3/5, Loss: 0.0175\n",
      "Client 2, Epoch 4/5, Loss: 0.0045\n",
      "Client 2, Epoch 5/5, Loss: 0.0070\n",
      "Client Accuracies: ['100.00%', '100.00%', '99.93%']\n",
      "Global Validation Accuracy: 100.00%\n",
      "Global Validation Loss: 0.0000\n",
      "\n",
      "Communication Round 5\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 0 features shape: torch.Size([32, 1, 121])\n",
      "Client 0, Epoch 1/5, Loss: 0.0000\n",
      "Client 0, Epoch 2/5, Loss: 0.0076\n",
      "Client 0, Epoch 3/5, Loss: 0.0002\n",
      "Client 0, Epoch 4/5, Loss: 0.0000\n",
      "Client 0, Epoch 5/5, Loss: 0.0000\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 1 features shape: torch.Size([32, 1, 121])\n",
      "Client 1, Epoch 1/5, Loss: 0.0000\n",
      "Client 1, Epoch 2/5, Loss: 0.0000\n",
      "Client 1, Epoch 3/5, Loss: 0.0000\n",
      "Client 1, Epoch 4/5, Loss: 0.0000\n",
      "Client 1, Epoch 5/5, Loss: 0.0259\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "DEBUG - Client 2 features shape: torch.Size([32, 1, 121])\n",
      "Client 2, Epoch 1/5, Loss: 0.0003\n",
      "Client 2, Epoch 2/5, Loss: 0.0000\n",
      "Client 2, Epoch 3/5, Loss: 0.0000\n",
      "Client 2, Epoch 4/5, Loss: 0.0000\n",
      "Client 2, Epoch 5/5, Loss: 0.0000\n",
      "Client Accuracies: ['100.00%', '99.97%', '100.00%']\n",
      "Global Validation Accuracy: 100.00%\n",
      "Global Validation Loss: 0.0000\n",
      "\n",
      "Classification Report for Final Global Model Confusion Matrix:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        Adware       1.00      1.00      1.00      1034\n",
      "      Backdoor       1.00      1.00      1.00       105\n",
      "  FileInfector       1.00      1.00      1.00        27\n",
      "           PUA       1.00      1.00      1.00       121\n",
      "    Ransomware       1.00      1.00      1.00       311\n",
      "      Riskware       1.00      1.00      1.00      1356\n",
      "     Scareware       1.00      1.00      1.00        83\n",
      "        Trojan       1.00      1.00      1.00       786\n",
      " Trojan_Banker       1.00      1.00      1.00        25\n",
      "Trojan_Dropper       1.00      1.00      1.00       159\n",
      "    Trojan_SMS       1.00      1.00      1.00       187\n",
      "    Trojan_Spy       1.00      1.00      1.00       212\n",
      "\n",
      "      accuracy                           1.00      4406\n",
      "     macro avg       1.00      1.00      1.00      4406\n",
      "  weighted avg       1.00      1.00      1.00      4406\n",
      "\n",
      "\n",
      "Results saved to federated_learning_results\\final_results.txt\n",
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'data_path': r'C:\\Users\\USER\\Documents\\NTUST\\Conference_Workshop_Seminar\\Android\\Dataset\\AndMal2020-dynamic-BeforeAndAfterReboot\\Cleaned_Files\\normalized_dataset',\n",
    "    'num_clients': 3,\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 32,\n",
    "    'communication_rounds': 5,\n",
    "    'train_split_ratio': 0.8,  # 80% train, 20% validation\n",
    "    'num_ensemble_models': 3  # Number of models in the ensemble\n",
    "}\n",
    "\n",
    "# CNN Model for Android Malware Classification\n",
    "class MalwareCNN(nn.Module):\n",
    "    def __init__(self, input_features, num_classes):\n",
    "        super(MalwareCNN, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Calculate flattened size after convolutions\n",
    "        self.flattened_size = 64 * (input_features // 2 // 2)  # Two pooling layers with stride 2\n",
    "        \n",
    "        # Classification layers\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 512)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input shape [batch_size, 1, num_features]\n",
    "        if x.dim() != 3:\n",
    "            raise ValueError(f\"Expected input shape [batch_size, 1, num_features], got {x.shape}\")\n",
    "        \n",
    "        # Convolution layers\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Ensemble Model for Android Malware Classification\n",
    "class EnsembleMalwareCNN(nn.Module):\n",
    "    def __init__(self, input_features, num_classes, num_models):\n",
    "        super(EnsembleMalwareCNN, self).__init__()\n",
    "        self.models = nn.ModuleList([MalwareCNN(input_features, num_classes) for _ in range(num_models)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [model(x) for model in self.models]\n",
    "        avg_output = torch.mean(torch.stack(outputs), dim=0)\n",
    "        return avg_output\n",
    "\n",
    "# Improved Android Malware Dataset\n",
    "class AndroidMalwareDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.max_features = 0\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # First pass: determine the maximum number of features\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                if 'Category' not in df.columns:\n",
    "                    print(f\"'Category' column not found in {file_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Count features (excluding Category column)\n",
    "                num_features = df.select_dtypes(include=[np.number]).drop(['Category'], axis=1, errors='ignore').shape[1]\n",
    "                self.max_features = max(self.max_features, num_features)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error checking features in {file_path}: {e}\")\n",
    "        \n",
    "        print(f\"Maximum feature dimension across all files: {self.max_features}\")\n",
    "        \n",
    "        # Collect all feature data for standardization\n",
    "        all_features = []\n",
    "        \n",
    "        # Second pass: load data and collect for standardization\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                if 'Category' not in df.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Process features: Keep only numerical columns\n",
    "                features_df = df.select_dtypes(include=[np.number]).drop(['Category'], axis=1, errors='ignore')\n",
    "                features = features_df.values\n",
    "                \n",
    "                # Pad features to match max_features for standardization\n",
    "                if features.shape[1] < self.max_features:\n",
    "                    padding = np.zeros((features.shape[0], self.max_features - features.shape[1]))\n",
    "                    features = np.hstack((features, padding))\n",
    "                \n",
    "                all_features.append(features)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error collecting features from {file_path}: {e}\")\n",
    "        \n",
    "        # Combine and standardize all features\n",
    "        if all_features:\n",
    "            combined_features = np.vstack(all_features)\n",
    "            self.scaler.fit(combined_features)\n",
    "        \n",
    "        # Third pass: standardize, pad and add to dataset\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                if 'Category' not in df.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Process features: Keep only numerical columns\n",
    "                features_df = df.select_dtypes(include=[np.number]).drop(['Category'], axis=1, errors='ignore')\n",
    "                features = features_df.values\n",
    "                \n",
    "                # Pad features to match max_features\n",
    "                if features.shape[1] < self.max_features:\n",
    "                    padding = np.zeros((features.shape[0], self.max_features - features.shape[1]))\n",
    "                    features = np.hstack((features, padding))\n",
    "                    print(f\"Padded features in {file_path} from {features_df.shape[1]} to {self.max_features}\")\n",
    "                \n",
    "                # Standardize features\n",
    "                features = self.scaler.transform(features)\n",
    "                \n",
    "                # Extract labels from the 'Category' column\n",
    "                labels = df['Category'].values\n",
    "                \n",
    "                # Add to dataset\n",
    "                for i in range(features.shape[0]):\n",
    "                    self.data.append(torch.tensor(features[i].astype(np.float32)))\n",
    "                    self.labels.append(labels[i])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "        \n",
    "        # Convert category names to numerical labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.encoded_labels = self.label_encoder.fit_transform(self.labels)\n",
    "        self.num_classes = len(self.label_encoder.classes_)\n",
    "        \n",
    "        print(f\"Loaded {len(self.data)} samples across {self.num_classes} malware families\")\n",
    "        print(f\"Each sample has {self.max_features} features\")\n",
    "        print(f\"Malware families: {self.label_encoder.classes_}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.encoded_labels[idx]\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        features, labels = zip(*batch)\n",
    "        # Stack features with correct dimensions (fixed)\n",
    "        features_stacked = torch.stack([f for f in features], dim=0)\n",
    "        # Add channel dimension correctly for CNN input\n",
    "        features_stacked = features_stacked.unsqueeze(1)  # [batch_size, 1, features]\n",
    "        return features_stacked, torch.tensor(labels)\n",
    "\n",
    "# Federated Learning Server\n",
    "class FederatedServer:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.global_model = EnsembleMalwareCNN(input_features=dataset.max_features, \n",
    "                                               num_classes=dataset.num_classes,\n",
    "                                               num_models=CONFIG['num_ensemble_models'])\n",
    "        self.global_accuracy_history = []\n",
    "        self.global_loss_history = []\n",
    "        \n",
    "        # Prepare distributed datasets for clients\n",
    "        self.client_datasets = self._prepare_client_datasets()\n",
    "    \n",
    "    def _prepare_client_datasets(self):\n",
    "        # Split into train and validation\n",
    "        train_size = int(CONFIG['train_split_ratio'] * len(self.dataset))\n",
    "        val_size = len(self.dataset) - train_size\n",
    "        train_dataset, val_dataset = random_split(self.dataset, [train_size, val_size])\n",
    "        \n",
    "        # Distribute training data among clients\n",
    "        client_datasets = []\n",
    "        total_train_samples = len(train_dataset)\n",
    "        samples_per_client = total_train_samples // CONFIG['num_clients']\n",
    "        \n",
    "        for i in range(CONFIG['num_clients']):\n",
    "            start_idx = i * samples_per_client\n",
    "            end_idx = start_idx + samples_per_client if i < CONFIG['num_clients'] - 1 else total_train_samples\n",
    "            client_subset = torch.utils.data.Subset(train_dataset, range(start_idx, end_idx))\n",
    "            client_datasets.append(client_subset)\n",
    "        \n",
    "        return {\n",
    "            'train_datasets': client_datasets,\n",
    "            'validation_dataset': val_dataset\n",
    "        }\n",
    "    \n",
    "    def distribute_model(self):\n",
    "        return self.global_model.state_dict()\n",
    "    \n",
    "    def aggregate_weights(self, client_weights):\n",
    "        global_dict = self.global_model.state_dict()\n",
    "        \n",
    "        for key in global_dict.keys():\n",
    "            global_dict[key] = torch.stack([\n",
    "                client_weights[i][key].float() for i in range(len(client_weights))\n",
    "            ], 0).mean(0)\n",
    "        \n",
    "        self.global_model.load_state_dict(global_dict)\n",
    "    \n",
    "    def evaluate_global_model(self):\n",
    "        self.global_model.eval()\n",
    "        val_loader = DataLoader(self.client_datasets['validation_dataset'], \n",
    "                               batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=AndroidMalwareDataset.collate_fn)\n",
    "        \n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                outputs = self.global_model(features)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                total_samples += labels.size(0)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "                total_loss += loss.item() * labels.size(0)\n",
    "        \n",
    "        accuracy = total_correct / total_samples * 100 if total_samples > 0 else 0\n",
    "        avg_loss = total_loss / total_samples if total_samples > 0 else float('inf')\n",
    "        \n",
    "        self.global_accuracy_history.append(accuracy)\n",
    "        self.global_loss_history.append(avg_loss)\n",
    "        \n",
    "        return accuracy, avg_loss\n",
    "    \n",
    "    def generate_confusion_matrix(self, title=\"Global Model Confusion Matrix\"):\n",
    "        self.global_model.eval()\n",
    "        val_loader = DataLoader(self.client_datasets['validation_dataset'], \n",
    "                               batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=AndroidMalwareDataset.collate_fn)\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                outputs = self.global_model(features)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_preds.extend(predicted.numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        # Convert numeric labels back to family names for better readability\n",
    "        class_names = self.dataset.label_encoder.classes_\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=class_names,\n",
    "                   yticklabels=class_names)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{title.replace(\" \", \"_\")}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\nClassification Report for {title}:\")\n",
    "        print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    def run_federated_learning(self):\n",
    "        print(\"\\nClient Dataset Sizes:\")\n",
    "        for client_id, client_dataset in enumerate(self.client_datasets['train_datasets']):\n",
    "            print(f\"Client {client_id}: {len(client_dataset)} samples\")\n",
    "        \n",
    "        round_client_accuracies = []\n",
    "        \n",
    "        for round in range(CONFIG['communication_rounds']):\n",
    "            print(f\"\\nCommunication Round {round + 1}\")\n",
    "            \n",
    "            client_weights = []\n",
    "            client_accuracies = []\n",
    "            \n",
    "            for client_id, client_dataset in enumerate(self.client_datasets['train_datasets']):\n",
    "                client_model = EnsembleMalwareCNN(input_features=self.dataset.max_features, \n",
    "                                                  num_classes=self.dataset.num_classes,\n",
    "                                                  num_models=CONFIG['num_ensemble_models'])\n",
    "                client_model.load_state_dict(self.distribute_model())\n",
    "                \n",
    "                # Train client model\n",
    "                client_model.train()\n",
    "                optimizer = optim.Adam(client_model.parameters(), lr=CONFIG['learning_rate'])\n",
    "                train_loader = DataLoader(client_dataset, batch_size=CONFIG['batch_size'], shuffle=True, \n",
    "                                         collate_fn=AndroidMalwareDataset.collate_fn)\n",
    "                \n",
    "                for epoch in range(CONFIG['epochs']):\n",
    "                    epoch_loss = 0\n",
    "                    for features, labels in train_loader:\n",
    "                        # Debug: print tensor shape to verify dimensions\n",
    "                        if epoch == 0 and epoch_loss == 0:\n",
    "                            print(f\"DEBUG - Client {client_id} features shape: {features.shape}\")\n",
    "                            \n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = client_model(features)\n",
    "                        loss = F.cross_entropy(outputs, labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        epoch_loss += loss.item()\n",
    "                    \n",
    "                    avg_epoch_loss = epoch_loss / len(train_loader) if len(train_loader) > 0 else float('inf')\n",
    "                    print(f\"Client {client_id}, Epoch {epoch+1}/{CONFIG['epochs']}, Loss: {avg_epoch_loss:.4f}\")\n",
    "                \n",
    "                # Evaluate client model\n",
    "                client_accuracy = self._evaluate_client_model(client_model, client_dataset)\n",
    "                client_accuracies.append(client_accuracy)\n",
    "                \n",
    "                # Send model weights to server\n",
    "                client_weights.append(client_model.state_dict())\n",
    "            \n",
    "            # Aggregate model weights and update global model\n",
    "            self.aggregate_weights(client_weights)\n",
    "            global_accuracy, global_loss = self.evaluate_global_model()\n",
    "            round_client_accuracies.append(client_accuracies)\n",
    "            \n",
    "            print(f\"Client Accuracies: {[f'{acc:.2f}%' for acc in client_accuracies]}\") \n",
    "            print(f\"Global Validation Accuracy: {global_accuracy:.2f}%\")\n",
    "            print(f\"Global Validation Loss: {global_loss:.4f}\")\n",
    "        \n",
    "        # Generate final confusion matrix\n",
    "        self.generate_confusion_matrix(\"Final Global Model Confusion Matrix\")\n",
    "        \n",
    "        # Plot learning curves\n",
    "        self._plot_learning_curves(round_client_accuracies)\n",
    "        \n",
    "        return global_accuracy\n",
    "    \n",
    "    def _evaluate_client_model(self, model, dataset):\n",
    "        model.eval()\n",
    "        dataloader = DataLoader(dataset, batch_size=CONFIG['batch_size'], shuffle=False, collate_fn=AndroidMalwareDataset.collate_fn)\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, labels in dataloader:\n",
    "                outputs = model(features)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        return correct / total * 100 if total > 0 else 0\n",
    "    \n",
    "    def _plot_learning_curves(self, round_client_accuracies):\n",
    "        rounds = range(1, CONFIG['communication_rounds'] + 1)\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Accuracy Plot\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(rounds, self.global_accuracy_history, label='Global Accuracy', \n",
    "                color='blue', marker='o', linewidth=2)\n",
    "        \n",
    "        client_colors = ['red', 'green', 'purple', 'orange', 'brown']\n",
    "        for client_id in range(min(CONFIG['num_clients'], len(client_colors))):\n",
    "            client_round_accuracies = [round_accuracies[client_id] for round_accuracies in round_client_accuracies]\n",
    "            plt.plot(rounds, client_round_accuracies,\n",
    "                    label=f'Client {client_id} Accuracy',\n",
    "                    color=client_colors[client_id],\n",
    "                    linestyle='--',\n",
    "                    marker='x')\n",
    "        \n",
    "        plt.title('Accuracy vs Communication Rounds')\n",
    "        plt.xlabel('Communication Rounds')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Loss Plot\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(rounds, self.global_loss_history, label='Global Loss', \n",
    "                color='blue', marker='o', linewidth=2)\n",
    "        plt.title('Loss vs Communication Rounds')\n",
    "        plt.xlabel('Communication Rounds')\n",
    "        plt.ylabel('Cross Entropy Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('federated_learning_curves.png')\n",
    "        plt.close()\n",
    "\n",
    "# Main execution block\n",
    "def main():\n",
    "    print(\"Android Malware Detection using Federated Learning\")\n",
    "    print(\"-------------------------------------------------\")\n",
    "    \n",
    "    results_dir = 'federated_learning_results'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"\\nLoading Android malware datasets...\")\n",
    "    # Load all CSV files with 'after_reboot' in the filename\n",
    "    files = glob.glob(CONFIG['data_path'] + '/*after_reboot*.csv')\n",
    "    print(f\"Found {len(files)} files matching the pattern\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = AndroidMalwareDataset(files)\n",
    "    \n",
    "    if dataset.num_classes == 0:\n",
    "        print(\"Error: Failed to load dataset correctly. Check the file paths and data format.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"Number of samples: {len(dataset)}\")\n",
    "    print(f\"Number of features: {dataset.max_features}\")\n",
    "    print(f\"Number of malware families: {dataset.num_classes}\")\n",
    "    \n",
    "    print(\"\\nInitializing Federated Learning Server...\")\n",
    "    server = FederatedServer(dataset)\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nStarting Federated Learning Training...\")\n",
    "        final_accuracy = server.run_federated_learning()\n",
    "        \n",
    "        # Save results\n",
    "        results_file = os.path.join(results_dir, 'final_results.txt')\n",
    "        with open(results_file, 'w') as f:\n",
    "            f.write(f\"Final Validation Accuracy: {final_accuracy:.2f}%\\n\")\n",
    "            f.write(f\"Number of malware families: {dataset.num_classes}\\n\")\n",
    "            f.write(f\"Malware families: {', '.join(dataset.label_encoder.classes_)}\\n\")\n",
    "        \n",
    "        print(f\"\\nResults saved to {results_file}\")\n",
    "        print(\"Training completed successfully!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
